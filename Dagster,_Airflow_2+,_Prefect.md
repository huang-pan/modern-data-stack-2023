# Dagster, Airflow 2+, Prefect, Snowflake

## Dagster

- Dagster
    - Type\-checked, **composable** pipeline definitions
        - declarative \(Airflow imperative\)
            - a lot less duplicate code than Airflow
    	- ****imperative****: explicitly define operations
     	- ****declarative****: define the outcomes you want
    - Automatic tracking of dependencies between tasks
    - Built\-in data validation and error handling
    - Integration with ML frameworks like TensorFlow and PyTorch
    - Strong emphasis on testing and reproducibility
        - great for CI / CD, local dev / test
    - dbt model pipeline auto materialization upon one upstream model change
    	- https://youtube.com/watch?v=2yuyIl31ypM&si=hDBFgNc0NnVhTmQX
- tutorials
    - [https://dagster.io/blog/dagster\-crash\-course\-oct\-2022](https://dagster.io/blog/dagster-crash-course-oct-2022) 
    - [https://www.youtube.com/watch?v=L5kTxCM\-tOk](https://www.youtube.com/watch?v=L5kTxCM-tOk)
        - The problem Dagster solves. Dagster's pros over Airflow
    - [https://www.youtube.com/watch?v=t8QADtYdWEI](https://www.youtube.com/watch?v=t8QADtYdWEI)
        - sample data pipeline with dagster
        - ops \(tasks\), graphs \(DAG of ops or software defined assets\), jobs \(parameterized graphs\), repository \(collection of jobs\), workspace \(collection of repos\), scheduling, testing. sensor
    - [https://www.youtube.com/watch?v=UrjHVJ2GZN0](https://www.youtube.com/watch?v=UrjHVJ2GZN0)
        - easy local development / test environment with Dagster \(and Prefect\)
        - dagster architecture
            - launcher: k8s, ECS, docker, celeryK8s
            - executor\(s\)
        - dagster.yaml
            - defines dagster deployment
        - workspace.yaml
            - maintain workspaces
        - resource
            - external dependencies \(e.g. database connection, API, etc.\)
            - hides keys, passwords, etc.
            - dev, prod
                - etl\_local: mock database, etc.
                - etl\_docker: real database, etc.
        - asset
            - an entity that is created or mutated by an op, e.g. database table, ML model, AWS resource, etc.
            - Dagster UI has **asset catalog**
        - **tests can be done at the op or graph level**
        - backfills
            - **backfills fully supported in Dagster UI**
            - [https://dagster.io/blog/backfills\-in\-ml](https://dagster.io/blog/backfills-in-ml) 
        - partitioned data pipelines
            - [https://youtu.be/LFOikWqCOAM](https://youtu.be/LFOikWqCOAM)
        - IO management
        - graphQL layer
            - data pipeline / lineage search feature
            - [https://youtu.be/MPouVV40wBs](https://youtu.be/MPouVV40wBs)
    - [https://www.youtube.com/watch?v=pRGNxIz6GzU](https://www.youtube.com/watch?v=pRGNxIz6GzU)
        - Dagster is run off of **software defined assets**
            - a declarative, pure Python function that computes the value of an asset and has associated metadata.
            - **easier to compose graphs of software defined assets**
        - asset materialization
            - job run log, metadata
            - asset created or modified
            - [https://dagster.io/blog/dagster\-script\-to\-assets](https://dagster.io/blog/dagster-script-to-assets) 
        - 2 types of orchestration
            - DAGs of assets on a schedule
            - **declarative approach: reconciliation, no more low level imperative work**
                - fix any discrepancies with software defined assets
                    - e.g. upstream materialization changed, automatically change downstream materialization
                - [https://dagster.io/blog/declarative\-scheduling](https://dagster.io/blog/declarative-scheduling) 
                - [https://dagster.io/blog/declarative\-scheduling\-for\-dbt](https://dagster.io/blog/declarative-scheduling-for-dbt) 
        - [https://motherduck.com/](https://motherduck.com/) duckdb, poor man's data lake
        - [https://continual.ai/](https://continual.ai/) continual model improvement using Generative AI
            - no more low level work
- **Played around with Dagster Cloud**
    - [https://github.com/huang\-pan/dagster\_starter](https://github.com/huang-pan/dagster_starter) 
        - uses pytest

## Airflow vs. Dagster vs. Prefect

- https://dataengineeringcentral.substack.com/p/the-truth-about-prefect-mage-and
	- Prefect: meh
 	- Mage: made with dev experience in mind
- [https://www.restack.io/docs/airflow\-vs\-dagster](https://www.restack.io/docs/airflow-vs-dagster)
    - The TaskFlow API: One of the most important additions of the 2.0 release was the introduction of the TaskFlow API. Namely, it offers the ability to use decorators to declare DAGs \( @dag\) and tasks \( @task\), making it much easier to work with tasks as typical Python functions \(instead of the counter\-intuitive PythonOperator\) while also abstracting the complexities of XCOM to move small bits of data between tasks.
    - TaskGroups: As a data engineer working with Airflow, you’ve probably experienced the eternal struggle of navigating a more\-than\-100\-tasks DAG. With earlier versions of Airflow, the only possible option to improve this experience was the use of SubDAGs \(via the SubDagOperator\) which added unnecessary complexity because a SubDAG is technically yet one more DAG to manage. On the other hand, the TaskGroup concept introduced with Airflow 2.0 allows you to group a set of tasks without defining them as a separate DAG, making it much more convenient to use.
    - But on the darker side of things, **Airflow is still a system that’s difficult to test and maintain: defining and running unit tests for your tasks is still limited, while integration tests require a lot of moving pieces, and integrating it within a CI/CD pipeline necessitates a lot of complexity and friction.**
    - Dagster is rigid and opinionated, while Airflow is flexible and accommodating. 
        - Dagster is built for the modern data stack with its dbt models and Airbyte connectors in mind, 
        - while Airflow is built to orchestrate tasks within every stack that ever was and that ever will.
    - If you’re **building a new data platform today, Dagster** benefits from being the orchestrator that’s designed specifically for you. It loves interacting with the tools of the modern data stack and would allow you to get up and running in no time while also **streamlining your pipelines with its declarative approach**.
    - On the other hand, Airflow still boasts a bigger community. It’s the battle\-tested orchestrator that managed to withstand the test of time, and even though its armor might be scratched it’s still a trustworthy lieutenant. Sure, it’s too old to know that you’re running dbt models, but it’s also wise enough to manage any type of workflow you want to throw at it.
- [https://towardsdatascience.com/airflow\-prefect\-and\-dagster\-an\-inside\-look\-6074781c9b77](https://towardsdatascience.com/airflow-prefect-and-dagster-an-inside-look-6074781c9b77) 
    - The main issues we’ve seen with Airflow deployments fall into one of several categories:
        - local development, testing, and storage abstractions
            - problem with cloud provider managed airflow: lack of local testing
                - Astronomer solves?
                - **easy local dev / test environment with Dagster and Prefect**
            - CI / CD processes possible with managed Airflow
        - one\-off and irregularly scheduled tasks
            - this is a problem
            - Airflow 2.4 has data driven scheduling: DAG run on Dataset completion
        - the movement of data between tasks
            - use Xcoms for small data, databases / data stores for large data
        - dynamic and parameterized workflows
            - Airflow 2.3 has dynamic task mapping / generation
- [https://medium.com/codex/dagster\-vs\-apache\-airflow\-side\-by\-side\-comparison\-965149997cee\#:~:text=Apache%20Airflow%20is%20best%20for,frameworks%20like%20TensorFlow%20or%20PyTorch](https://medium.com/codex/dagster-vs-apache-airflow-side-by-side-comparison-965149997cee#:~:text=Apache%20Airflow%20is%20best%20for,frameworks%20like%20TensorFlow%20or%20PyTorch).
    - Choose Apache Airflow if you need to generate tasks dynamically or if you need to integrate with tools like Spark or Hadoop.
    - Choose Dagster if you need strong data validation and error handling, or if you need integration with ML frameworks like TensorFlow or PyTorch.
    - **Dynamic task generation:** Apache Airflow allows you to generate tasks dynamically based on data or other factors. Dagster does not have this feature, which can be a limitation in some use cases.
- [https://dagster.io/blog/dagster\-airflow](https://dagster.io/blog/dagster-airflow) 
- Dagster vs Prefect [https://www.restack.io/docs/dagster\-vs\-prefect](https://www.restack.io/docs/dagster-vs-prefect)
    - Different orchestration philosophies: 
        - Dagster is a **data\-driven** development lifecycle orchestration tool that includes pipeline development, deployment, observability, and monitoring solutions. 
        - Prefect, on the other hand, is a **task\-driven** workflow orchestration tool that offers a scheduling engine that inputs code to build distributed pipelines and orchestrates them.
    - Dagster Better At
        - Asset management: Dagster is a complete asset management solution that ingests all your data assets into a single surface and provides a base layer of **lineage, observability, and data quality monitoring** for your entire platform.
        - Better UI for debugging: Dagster provides a **better interface for locally debugging problems in large and complex data pipelines**,
        - Dagster **declarative**, data driven pipelines \(Airflow, prefect imperative, task driven pipelines\)
    - Prefect better at
        - easier to get up and running with Prefect
            - Dagster has a reputation for being extremely powerful. However, the learning curve is still too steep.
            - One reason for this is that Dagster is actually three things rolled into one: it’s a scheduler, an operational asset catalog, and a data transformation tool. 
        - Prefect cloud native
        - can have dynamic pipelines \(Dagster can't\)
    - Suitable Use Case: 
        - Dagster’s orchestration solutions make it ideal for holistically solving **complex data engineering problems**. 
        - Whereas, Prefect is more suited to projects requiring **faster, dynamic orchestration**.
- Prefect
    - Prefect ML pipeline
        - [https://medium.com/@haythemtellili/ml\-workflow\-orchestration\-ddc8165c26c0](https://medium.com/@haythemtellili/ml-workflow-orchestration-ddc8165c26c0) 

## Airflow 2\+:

- Airflow Architecture
	- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/overview.html 
		- Astronomer Airflow 2 deploy modes: celery (fast execution), KubernetesPod (slower, scalable, isolation)
  		- Airflow on Kubernetes: https://youtube.com/watch?v=Vg75zZ6_9hU&si=hivWYZOEYOyz_1Fo
    		- Things to consider when building an Airflow service: https://youtube.com/watch?v=Q7H0JYVnI9I&si=XZsaXZrkfdXP6ee8
      		- Testing Airflow with Dagtest: https://youtube.com/watch?v=wkAb8mDDHMs&si=Ly_fxW-VpCuUtRWe
	- Executor types
		- Sequential Executor https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/sequential.html 
			- local, no parallelism
		- Local Executor https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/local.html
			- has parallelism
		- Celery Executor https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html
			- celery backend: rabbitmq, redis, etc.
			- https://www.accionlabs.com/how-to-setup-airflow-multinode-cluster-with-celery-rabbitmq
			- can use this in conjunction with Airflow on K8s: have set number  K8s worker pods, communicate with them using celery message queue (as with Juvo)
		- Kubernetes Executor https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/kubernetes.html
			- The Kubernetes executor runs each task instance in its own pod on a Kubernetes cluster.
			- Uses metadatadb, no need for celery queue (Greenstand)
			- Can install using Airflow Helm Chart (see Greenstand work with Helm / Ansible)
- Premade hooks, operators
	- https://registry.astronomer.io/ 
- 
- Airflow
    - Task\-based workflow definition
    - Dynamic task generation
    - Built\-in operators for common tasks \(e.g., PythonOperator, BashOperator, etc.\)
    - Web\-based user interface for monitoring and managing workflows
    - Large community and ecosystem of plugins and integrations
    - most data engineering teams built their **custom operators which turned into unnecessary liabilities and technical debt.**
- What's new in Airflow 2.0 [https://medium.com/hashmapinc/whats\-new\-with\-airflow\-2\-0\-27ae5fc9068a](https://medium.com/hashmapinc/whats-new-with-airflow-2-0-27ae5fc9068a)
    - multi schedulers for multi webservers
        - scheduler much more efficient than before
    - DAG serialization only needs to be present on scheduler
    - @task, @dag like prefect
    - smart sensors
- what's new in Airflow 2.3 [https://airflowsummit.org/slides/2022/l5\-Airflow2\_3\-Kaxil.pdf](https://airflowsummit.org/slides/2022/l5-Airflow2_3-Kaxil.pdf)
    - dynamic task mapping
    - grid view
    - events timetable: run DAGs at arbitrary dates
- what's new in Airflow 2.4 https://www.astronomer.io/blog/apache-airflow-2-4-everything-you-need-to-know/
	- data driven scheduling: DAG run on ****Dataset**** completion
		- https://youtube.com/watch?v=kPI2mPs-eQA&si=YeI0Skt1zmlWVuW4
  	- dynamic dags
- Airflow 2.7
	- What's new in Airflow 2.7 https://youtube.com/watch?v=uB7zweaF8EA&si=0cG6PbjcqsMLwl4C 
		- minor cosmetic changes
		- EMR cluster setup / teardown tasks
	- Tight integration with dbt https://youtube.com/watch?v=MhCuxTDlVkE&si=4xUGl-xXhxP6kspX 
		- airflow bash command: dbt run, dbt test (old way)
		- airflow call dbt cloud through API (needs dbt cloud subscription)
		- cosmos https://astronomer.github.io/astronomer-cosmos/index.html: bring dbt transforms into Airflow (best way)
		- bring dbt project (e.g. jaffle shop) into ****astronomer airflow dag git repo / folder scaffolding****
	- Testing Airflow with Dagtest: https://youtube.com/watch?v=wkAb8mDDHMs&si=Ly_fxW-VpCuUtRWe
	- Soda testing framework: Airflow + Soda.io + Snowflake 
		- https://youtube.com/watch?v=YZTcIi5o7FI&si=hCVPcuLsrWwRl13M 
		- python based testing of snowflake transforms, no need to use dbt
		- https://quickstarts.snowflake.com/guide/soda/index.html?index=..%2F..index#0
- Astronomer.io
	- https://youtube.com/watch?v=JI-m9Spr7sg&si=YksZgfS6fD0xDfmL 
	- best way to run managed Airflow
	- ****Airflow DAG repo / folder scaffolding****
		- dags
		- include
		- test
		- requirements
	- easy push to dev Airflow environment
		- Easy DAG creation UI in astronomer.io
	- Github CI / CD to prod Airflow environment
- ways to install Airflow
    - https://seattledataguy.substack.com/p/mistakes-i-have-seen-when-data-teams
  		- don't put dags, webserver, scheduler in same repo
		- use premade hooks, operators, sensors, variables
		- types of executors: sequential, local, celery, kubernetes, debug
		- scale! https://shopify.engineering/lessons-learned-apache-airflow-scale
			- Airflow logs can be on S3, but it's slow
			- reduce metadata amount
			- load distribution
				- pools: limits concurrency for a set of tasks
				- priority weight for a task
				- different celery queues for isolated workers
    - forgot details of Airflow scheduler using **celery message queue using rabbitMQ message bus** \(can also use **redis** like at Juvo\) to send data to workers
    - [https://airflow.apache.org/docs/apache\-airflow/stable/installation/index.html](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html)
        - pip, docker, helm, managed services
        - production deployment: [https://airflow.apache.org/docs/apache\-airflow/stable/administration\-and\-deployment/production\-deployment.html](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/production-deployment.html)
    - How to set up multi\-node Airflow cluster using RabbitMQ [https://www.accionlabs.com/how\-to\-setup\-airflow\-multinode\-cluster\-with\-celery\-rabbitmq](https://www.accionlabs.com/how-to-setup-airflow-multinode-cluster-with-celery-rabbitmq)
    - [https://airflow.apache.org/ecosystem/](https://airflow.apache.org/ecosystem/)
        - [Self\-Managed Airflow via CNDI](https://github.com/polyseam/cndi) \- Toolkit for deploying Airflow Kubernetes clusters, with support for AWS, GCP, Azure, VMWare, Bare\-Metal, and even multi/hybrid cloud support. See [docs](https://docs.cndi.run) for more details.
        - [Self\-managed Airflow on Amazon EKS](https://github.com/awslabs/data-on-eks/tree/main/schedulers/terraform/managed-airflow-mwaa) \- Self\-managed Apache Airflow deployment on Aamzon EKS using [Terraform](https://www.terraform.io/).
        - [Amazon provider package health dashboard](https://aws-mwaa.github.io/open-source/system-tests/dashboard.html) \- Dashboard listing all system tests within the Amazon provider package and their current health status: last execution status \(succeeded/failed, average duration, …\).
- Airflow executors [https://airflow.apache.org/docs/apache\-airflow/stable/core\-concepts/executor/index.html](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html)
    - sequentialexecutor \(default, one task at a time\), localexecutor \(has parallel local processes\)
    - **celeryexecutor**: celery backend \(rabbitMQ, redis\): Juvo's redis backend kept on getting clogged with unrun tasks, so I had to clear it manually
        - celery flower: web UI to monitor workers
        - https://airflow.apache.org/docs/apache-airflow/stable/security/flower.html
    - dask executor
    - kubernetesexecutor: we were going to upgrade Juvo from the celeryexecutor to k8sexecutor
        - also has k8spodoperator
    - celeryk8s, localk8s
- Airflow logging [https://airflow.apache.org/docs/apache\-airflow/stable/administration\-and\-deployment/logging\-monitoring/logging\-architecture.html](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.html)
- monitoring
    - Check Airflow health status using HTTP calls to webserver, scheduler: [https://airflow.apache.org/docs/apache\-airflow/stable/administration\-and\-deployment/logging\-monitoring/check\-health.html](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html)
    - MWAA: use AWS cloudwatch
    - prometheus / grafana
    - Datadog
- Airflow clusters
    - local / dev / prod clusters: Greenstand set up
    - dev / QA / prod cluster
        - separated by IAM role? different types of encryption
    - multiple schedulers
        - explained Juvo celery executor: used redis instead of RabbitMQ
    - packaging different versions
        - python pip requirements \-\-\> Dockerfile \-\> image
    - sensitive information: how to encrypt? Encrypt in transit, streaming, ftp, some type of SSL encryption
        - Juvo S3 bucket with tight security measures / IAM role
    - Juvo custom operators / hooks / sensors
    - explained usage of Spark \(dataframe transformation, write back to S3 / redshift\)
    - debug spark: first go into Spark job history UI, look at logical plan
        - spark submit shell \(wrapper\): custom Spark jobs at run time
    - Airflow S3 File sensor: https://medium.com/@satadru1998/mastering-data-workflow-precision-and-control-with-airflows-s3-file-sensor-6869d60f099d
- Jenkins: get up and running on localhost:9090

## Mage
- https://youtube.com/live/JKALtxziBG0?si=o3lTVcVFt70LcpLK 
	- Mage easy imperative pipeline dev env
	- Integrated with vs code
	- Can dev and visualize dbt models 
	- Unit testing of pipeline steps out of the box 
	- Still early stages

## Neo4j
- https://neo4j.com/developer/graph-data-science/graph-algorithms/
- https://neo4j.com/blog/graph-algorithms-neo4j-15-different-graph-algorithms-and-what-they-do/

## Snowflake

- https://docs.snowflake.com/en/guides-overview 
- features
	- https://docs.snowflake.com/en/user-guide/tables-external-intro
	- https://docs.snowflake.com/en/user-guide/object-clone 
	- schema evolution
		- https://docs.snowflake.com/en/user-guide/data-load-schema-evolution
  		- This feature is limited to COPY INTO <table> statements and Snowpipe data loads. INSERT operations cannot evolve the target table schema automatically.
    		- Snowpipe Streaming data loads are not supported with schema evolution.
	- snowpipe, snowpipe streaming
	- tasks
		- https://docs.snowflake.com/en/user-guide/tasks-intro
		- A task can execute any one of the following types of SQL code:
			- Single SQL statement
			- Call to a stored procedure
			- Procedural logic using Snowflake Scripting
		- Pipelines: Tasks can be combined with table streams for continuous ELT workflows to process recently changed table rows. Streams ensure exactly once semantics for new or changed data in a table.
  		- https://julielovesdata.medium.com/value-passing-in-snowflake-tasks-3a19e10a30a4
	- Dynamic tables
		- https://docs.snowflake.com/en/user-guide/dynamic-tables-about
			- https://youtu.be/4O2_I25izIs?si=Mt_dBHBr6kM9puL2 
			- streams and tasks: imperative way of creating pipelines
				- SQL can be optimized more than dynamic tables
				- can use serverless tasks
			- dynamic tables: declarative way of creating pipelines
				- figures out merge / insert / delete statement
				- best for incremental refresh tables, not suitable for very large tables
				- no serverless option
			- auto pipeline, replaces streams and tasks
			- no schema evolution
		- https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison
	- https://coalesce.io/ dbt for Snowflake
 	- Snowflake can read Iceberg in external storage space
  		- https://youtube.com/watch?v=Ix2BZV1AjxA&si=th1tfQ9KDZWFsDaG
    		- Need separate AWS Glue Catalog in conjunction with Snowflake Catalog
- performance
	- warehouse structure
		- 1 extra small snowflake warehouse for Airflow jobs
			- fewest number of queries
			- query planner to see if long running queries are more efficient
				- break down queries, add intermediate tables
			- periodic COPY INTO cheaper than Snowpipe
		- 1 small / medium for analysts, data scientists
		- 1 small / medium for Sigma Computing
	- warehouse auto-scaling
	- multicluster warehouses
	- caching
	- query acceleration service
		- https://docs.snowflake.com/en/user-guide/query-acceleration-service
	- search optimization service
		- https://docs.snowflake.com/en/user-guide/search-optimization-service
- optimization
	- https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions
		- tables are closed source column store format, micro-partitioned
	- set table clustering keys (expensive compute to recluster)
		- https://docs.snowflake.com/en/user-guide/tables-clustering-keys
		- https://docs.snowflake.com/en/user-guide/tables-auto-reclustering
	- query profiling
		- https://docs.snowflake.com/en/user-guide/ui-query-profile
		- https://docs.snowflake.com/en/user-guide/performance-query-exploring
		- https://docs.snowflake.com/en/user-guide/performance-query-warehouse
	- https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics
		- split tables, use materialized views
		- optimize queries
			- OBT, but only select needed columns in queries
				- use LIMIT in queries
	- https://medium.com/@gupta.sahil.201191/snowflake-performance-optimization-techniques-9d135e07ef37
 	- dbt runs use up a lot of snowflake compute; consider SQLmesh instead
- RBAC
	- https://hevodata.com/learn/snowflake-roles/#3
		- orgadmin, accountadmin, securityadmin, sysadmin, etc.
	- Row Access Policies https://docs.snowflake.com/en/user-guide/security-row-intro 
	- Dynamic Data Masking down to column level https://docs.snowflake.com/en/user-guide/security-column-ddm-intro 
- Warehouse load monitoring, cost controls
	- https://docs.snowflake.com/en/user-guide/warehouses-load-monitoring
	- https://docs.snowflake.com/en/guides-overview-cost 
		- https://docs.snowflake.com/en/user-guide/cost-controlling 
	- Sigma has Snowflake Cost Monitoring Dashboard
		- https://www.sigmacomputing.com/interactive-demos/snowflake-cost-monitoring-template
	- https://sonra.io/snowflake/7-guardrails-against-common-mistakes-that-inflate-snowflake-credit-usage/
- Data Lineage
	- https://www.phdata.io/blog/ultimate-guide-to-data-lineage-directly-in-snowflake/
 	- https://www.metaplane.dev/blog/the-definitive-guide-to-snowflake-data-lineage
  	- https://community.snowflake.com/s/question/0D53r0000BZbxJgCQJ/how-do-you-address-data-lineage
  		- coalesce, dbt, atlan, databand.ai, etc.
- Dynamic Data pipelines with SQL Macros (like dbt macros)
	- https://youtu.be/4O2_I25izIs?si=Mt_dBHBr6kM9puL2
![dynamicpipeline](https://github.com/huang-pan/modern-data-stack-2023/assets/10567714/ce7db7eb-d24e-47a1-aaef-daf2c5f881e7)
![sqlmacro](https://github.com/huang-pan/modern-data-stack-2023/assets/10567714/57df1e49-9081-4cc6-a16f-3f9d53863d55)
![dynamicsql](https://github.com/huang-pan/modern-data-stack-2023/assets/10567714/d629c549-b7a9-46c5-8906-68027caf125c)
